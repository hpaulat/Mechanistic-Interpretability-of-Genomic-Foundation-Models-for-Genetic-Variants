{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f9eb05",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4b6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# NOTE:\n",
    "# This notebook assumes execution from the project root.\n",
    "# It explicitly sets cwd to external/genomic-FM so GV-Rep loaders work.\n",
    "\n",
    "os.chdir(\"../external/genomic-FM\")\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from src.dataloader.data_wrapper import RealClinVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd07932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mechanistic_interpretability/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5974e1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Records: 155\n",
      "Record length: 2\n",
      "Example reference sequence: CTTTCTTGCGGAGATTCTCTTCCTCTGTGCGCCGGTCTCTCCCAGGACAGGCACAAACACGCACCTCAAAGCTGTTCCGTCCCAGTAGATTACCACTACTCAGGATAGGAAAAGAGAAGCAAGAGGCAGTAAGGAAATCAGGTCCTACCTGTCCCATTTAAAAAACCAGGCTCCATCTACTCCCAACCACCCTTGTCCTTTCTGGAGCCTAAGCTCCAGCTCCAGGTAGGTGGAGGAGAAGCCACAGGTTAAGAGGTCCCAAAGCCAGAGAAAAGAAAACTGAGTGGGAGCAGTAAGGAGATTCCCCGCCGGGGATGTGATGAGAGGTGGATGGGTAGTAGTATGGAAGAAATCGGTAAGAGGTGGGCCCAGGGGTCAGAGGCAAGCAGAGGCTGGGGCACAGCAGGCCAGTGTGCAGGGTGGCAAGTGGCTCCTGACCTGGAGTCTTCCAGTGTGATGATGGTGAGGATGGGCCTCCGGTTCATGCCGCCCATGCAGGAACTGTTACACATGTAGTTGTAGTGGATGGTGGTACAGTCAGAGCCAACCTAGGAGATAACACAGGCCCAAGATGAGGCCAGTGCGCCTTGGGGAGACCTGTGGCAAGCAGGGGAGGCCTTTTTTTTTTTTTTTTGAGATGGAATCTCGCTCTGTCGCCCAGGCTGGAGTGCAGTGGCGTGATCTCAGCTCACTGCAAGCTCCACCGCCCAGGTTCACGCCATTCTCCTTCCTCAGCCTCCCGAGTAGCTGGGACTACAGGTGCCCAGCACCACGCCCGGCTAATTTTTTTTTGTATTTTTCAGTAGAGACGGGGTTTCACCGTTAGCCAGGATGGTCTCGATCTCCCAACCTCGTGATCCGCCTGCCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCACTGCGCCCAGCCAAGCAGGGGAGGCCCTTAGCCTCTGTAAGCTTCAGTTTTTTCAACTGTGCAATAGTTAAACCCATTTACTTTGCACATCTCATGGGGTTATAGGGAGGTCAAATAAG\n",
      "Example alternative sequence: CTTTCTTGCGGAGATTCTCTTCCTCTGTGCGCCGGTCTCTCCCAGGACAGGCACAAACACGCACCTCAAAGCTGTTCCGTCCCAGTAGATTACCACTACTCAGGATAGGAAAAGAGAAGCAAGAGGCAGTAAGGAAATCAGGTCCTACCTGTCCCATTTAAAAAACCAGGCTCCATCTACTCCCAACCACCCTTGTCCTTTCTGGAGCCTAAGCTCCAGCTCCAGGTAGGTGGAGGAGAAGCCACAGGTTAAGAGGTCCCAAAGCCAGAGAAAAGAAAACTGAGTGGGAGCAGTAAGGAGATTCCCCGCCGGGGATGTGATGAGAGGTGGATGGGTAGTAGTATGGAAGAAATCGGTAAGAGGTGGGCCCAGGGGTCAGAGGCAAGCAGAGGCTGGGGCACAGCAGGCCAGTGTGCAGGGTGGCAAGTGGCTCCTGACCTGGAGTCTTCCAGTGTGATGATGGTGAGGATGGGCCTCCGGTTCATGCCGCCCATGCAGGAACTGTTACACATTTAGTTGTAGTGGATGGTGGTACAGTCAGAGCCAACCTAGGAGATAACACAGGCCCAAGATGAGGCCAGTGCGCCTTGGGGAGACCTGTGGCAAGCAGGGGAGGCCTTTTTTTTTTTTTTTTGAGATGGAATCTCGCTCTGTCGCCCAGGCTGGAGTGCAGTGGCGTGATCTCAGCTCACTGCAAGCTCCACCGCCCAGGTTCACGCCATTCTCCTTCCTCAGCCTCCCGAGTAGCTGGGACTACAGGTGCCCAGCACCACGCCCGGCTAATTTTTTTTTGTATTTTTCAGTAGAGACGGGGTTTCACCGTTAGCCAGGATGGTCTCGATCTCCCAACCTCGTGATCCGCCTGCCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCACTGCGCCCAGCCAAGCAGGGGAGGCCCTTAGCCTCTGTAAGCTTCAGTTTTTTCAACTGTGCAATAGTTAAACCCATTTACTTTGCACATCTCATGGGGTTATAGGGAGGTCAAATAAG\n",
      "Unique labels in dataset: {'Class 4', 'Class 1', 'Class 3', 'Class 5', 'Class 2'}\n"
     ]
    }
   ],
   "source": [
    "loader = RealClinVar(num_records=156, all_records=False)\n",
    "lc_data = loader.get_data(Seq_length=1024)\n",
    "\n",
    "print(\"Number of Records:\", len(lc_data))\n",
    "print(\"Record length:\",len(lc_data[0]))  # each record is a tuple (sequence, label)\n",
    "\n",
    "lc_ref_sequences = [record[0][0] for record in lc_data]\n",
    "lc_alt_sequences = [record[0][1] for record in lc_data]\n",
    "\n",
    "lc_labels = [record[1] for record in lc_data]\n",
    "\n",
    "print(\"Example reference sequence:\", lc_ref_sequences[0])\n",
    "print(\"Example alternative sequence:\", lc_alt_sequences[0])\n",
    "print(\"Unique labels in dataset:\", set(lc_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20d2c9",
   "metadata": {},
   "source": [
    "Based on the average length, we can decide on a fixed length for tokenization. Because our tokenizer takes 6-mers, we can safely use 171 tokens and have all the sequences be properly tokenized.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\", trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-500m-multi-species\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24060d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily bottleneck sequences for testing on my laptop, later move to colab\n",
    "lc_sequences = lc_ref_sequences[:50]\n",
    "lc_labels = lc_labels[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde09d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape: torch.Size([50, 175]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# length to which the input sequences are padded\n",
    "max_length = 171    # 2048, hence 12kbp context window\n",
    "\n",
    "\n",
    "# Create a dummy dna sequence and tokenize it (6-mers if multiple of 6)\n",
    "tokens_ids = tokenizer.batch_encode_plus(lc_sequences, return_tensors=\"pt\", padding=\"max_length\", max_length = max_length)[\"input_ids\"]\n",
    "print(\"Tokens shape:\", tokens_ids.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63fe51f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Mask tensor([[True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        ...,\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True],\n",
      "        [True, True, True,  ..., True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "attention_mask = tokens_ids != tokenizer.pad_token_id\n",
    "print(\"Attention Mask\", attention_mask)\n",
    "\n",
    "torch_outs = model(\n",
    "    tokens_ids,\n",
    "    attention_mask=attention_mask,  # prevents attention to padding tokens\n",
    "    output_attentions = True,\n",
    "    encoder_attention_mask=attention_mask,\n",
    "    output_hidden_states=True       # to get all layer embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1adfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_states = torch_outs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342acbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Layer Embeddings shape: torch.Size([50, 175, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last Layer Embeddings shape:\", output_states[-1].shape)\n",
    "embeddings = output_states[-1]  # (B, L, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea4693",
   "metadata": {},
   "source": [
    "### Logistic Regression on Final Layer Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252eaff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels after binarization (0 for Class 1/2 and Class 1 for Class 3/4): {0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m embeddings_set = embeddings[:,\u001b[32m0\u001b[39m,:].detach().numpy()  \u001b[38;5;66;03m# use CLS token embeddings\u001b[39;00m\n\u001b[32m     10\u001b[39m X_train, X_test, y_train, y_test = train_test_split(embeddings_set, lc_labels, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mclf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining accuracy:\u001b[39m\u001b[33m\"\u001b[39m, clf.score(X_train, y_train))\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest accuracy:\u001b[39m\u001b[33m\"\u001b[39m, clf.score(X_test, y_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mechanistic_interpretability/lib/python3.11/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mechanistic_interpretability/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1243\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1240\u001b[39m     max_squared_sum = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1244\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1245\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1246\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m.classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1247\u001b[39m     )\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.warm_start:\n\u001b[32m   1250\u001b[39m     warm_start_coef = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcoef_\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lc_labels = [0 if label == \"Class 1\" or label == \"Class 2\" else 1 for label in lc_labels]  # binary classification\n",
    "print(\"Labels after binarization (0 for Class 1/2 and Class 1 for Class 3/4):\", set(lc_labels))\n",
    "\n",
    "# fit logistic regression on CLS token embeddings\n",
    "clf = linear_model.LogisticRegression(max_iter=1000)\n",
    "embeddings_set = embeddings[:,0,:].detach().numpy()  # use CLS token embeddings\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_set, lc_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training accuracy:\", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy:\", clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechanistic_interpretability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
